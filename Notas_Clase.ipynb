{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648124a2",
   "metadata": {},
   "source": [
    "# Data cleaning and Preparation\n",
    "\n",
    "## Handling missing data  / Manejo de los datos faltantes\n",
    "\n",
    "### ¿Que son los datos faltantes?\n",
    "\n",
    "Son valores que no se registraron, no se midieron, se perdieron o no aplican para una observación. Pasa por mil razones: encuestas sin responder, sensores que fallan, errores humanos, sistemas que no preguntan lo que debían.\n",
    "\n",
    "### ¿Como se presentan los datos faltantes?\n",
    "\n",
    "- NULL → el rey del “no hay nada aquí”.\n",
    "\n",
    "- NaN (Not a Number) → típico en Python, R y Excel para datos numéricos.\n",
    "\n",
    "- NA → muy usado en R.\n",
    "\n",
    "### ¿Cuales son los tipos de datos faltantes (Segun el origen o el motivo por el que faltan)?\n",
    "\n",
    "- MCAR (Missing Completely At Random - Faltantes completamente al azar)\n",
    "- MAR (Missing At Random - Faltantes al azar condicionado a otras variables)\n",
    "- MNAR (Missing Not At Random - Faltantes no al azar)\n",
    "\n",
    "### ¿Cuales son las tecnicas para manejar datos faltantes?\n",
    "\n",
    "Eliminación por filas (Listwise deletion)\n",
    "Eliminación por columnas\n",
    "Imputación simple (rápida, pero ingenua)\n",
    "Imputación basada en datos (más inteligente)\n",
    "KNN Imputation\n",
    "Imputación estadística avanzada\n",
    "MICE (Multiple Imputation by Chained Equations)\n",
    "\n",
    "\n",
    "### ¿Que es un algoritmo supervisado y no supervisado?\n",
    "- Algoritmos supervisados: Se entrenan con datos etiquetados, es decir, datos que ya tienen la respuesta correcta asociada. El objetivo es aprender una función que pueda predecir la etiqueta correcta para nuevos datos.\n",
    "Ejemplo: \n",
    "Clasificación y regresión. (Decision Trees, Random Forest, SVM, Redes Neuronales, etc.)\n",
    "Ejemplo de uso: Detección de spam, reconocimiento de imágenes, predicción de precios.\n",
    "\n",
    "- Algoritmos no supervisados: Trabajan con datos no etiquetados. El objetivo es encontrar patrones, estructuras o agrupaciones en los datos sin tener una respuesta correcta predefinida.\n",
    "Ejemplo:\n",
    "Clustering y reducción de dimensionalidad. (K-Means, DBSCAN, PCA, etc.)\n",
    "Ejemplo de uso: Segmentación de clientes, detección de anomalías, análisis exploratorio de datos.\n",
    "\n",
    "\n",
    "# K-Means Clustering\n",
    "\n",
    "### ¿Que es K-Means?\n",
    "\n",
    "K-Means es un algoritmo de aprendizaje no supervisado que agrupa datos en K clusters basándose en la similitud de sus características. El objetivo es minimizar la variabilidad dentro de cada cluster y maximizar la variabilidad entre clusters.\n",
    "\n",
    "### ¿Para que se usa K-Means?\n",
    "K-Means se utiliza para:\n",
    "- Segmentación de clientes\n",
    "- Agrupación de documentos\n",
    "- Detección de anomalías\n",
    "- Compresión de imágenes\n",
    "- Análisis exploratorio de datos\n",
    "\n",
    "### ¿Como aplicar K-Means en Python?\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "# Definir el número de clusters\n",
    "k = 3\n",
    "# Crear el modelo K-Means\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "# Ajustar el modelo a los datos\n",
    "kmeans.fit(data)\n",
    "# Obtener las etiquetas de los clusters\n",
    "labels = kmeans.labels_\n",
    "# Obtener los centroides de los clusters\n",
    "centroids = kmeans.cluster_centers_\n",
    "``` \n",
    "\n",
    "# DBSCAN Clustering\n",
    "\n",
    "### ¿Que es DBSCAN?\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de clustering basado en densidad que agrupa puntos que están densamente conectados y marca como ruido los puntos que están en regiones de baja densidad.\n",
    "\n",
    "### ¿Como aplicar DBSCAN en Python?\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "# Definir los parámetros de DBSCAN\n",
    "epsilon = 0.5  # Distancia máxima entre dos puntos para que se consideren vecinos\n",
    "min_samples = 5  # Número mínimo de puntos necesarios para formar un cluster\n",
    "# Crear el modelo DBSCAN\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "# Ajustar el modelo a los datos\n",
    "dbscan.fit(data)\n",
    "# Obtener las etiquetas de los clusters\n",
    "labels = dbscan.labels_\n",
    "```\n",
    "\n",
    "### ¿Para que se usa DBSCAN?\n",
    "DBSCAN se utiliza para:\n",
    "- Detección de anomalías\n",
    "- Agrupación de datos espaciales\n",
    "- Análisis de redes sociales\n",
    "- Segmentación de clientes\n",
    "- Análisis exploratorio de datos\n",
    "\n",
    "### ¿Cual es la diferencia entre K-Means y DBSCAN?\n",
    "- K-Means requiere que el número de clusters (K) sea especificado de antemano, mientras que DBSCAN determina automáticamente el número de clusters basándose en la densidad de los datos.\n",
    "- K-Means tiende a formar clusters de forma esférica y puede ser sensible a outliers, mientras que DBSCAN puede formar clusters de forma arbitraria y es más robusto frente a outliers.\n",
    "- K-Means utiliza la media de los puntos para definir el centro del cluster, mientras que DBSCAN utiliza la densidad de puntos para definir los clusters.\n",
    "\n",
    "### ¿Como idenfico cual usar?\n",
    "- Si se conoce el número de clusters y los datos son relativamente limpios, K-Means puede ser una buena opción.\n",
    "- Si los datos contienen ruido o outliers, o si los clusters tienen formas arbitrarias, DBSCAN puede ser más adecuado.\n",
    "\n",
    "### ¿Como se determina el numero de clusters en K-Means?\n",
    "- Método del codo (Elbow Method)\n",
    "- Silhouette Score\n",
    "- Gap Statistic\n",
    "\n",
    "### ¿Como se determina el numero de clusters en DBSCAN?\n",
    "- Parámetro epsilon (ε): Distancia máxima entre dos puntos para que se consideren vecinos\n",
    "- Parámetro minPts: Número mínimo de puntos necesarios para formar un cluster denso\n",
    "\n",
    "### ¿Como se determina epsilon y minPts en DBSCAN?\n",
    "- Epsilon (ε): Se puede determinar utilizando un gráfico de distancia k-vecino (k-distance graph) para identificar un punto de inflexión.\n",
    "- minPts: Una regla común es establecer minPts al menos igual a la dimensionalidad de los datos más uno (minPts ≥ D + 1).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PCA (Principal Component Analysis - Análisis de Componentes Principales)\n",
    "\n",
    "### ¿Que es PCA?\n",
    "\n",
    "PCA es una técnica estadística que transforma un conjunto de variables posiblemente correlacionadas en un conjunto de variables linealmente no correlacionadas llamadas componentes principales. El objetivo principal de PCA es reducir la dimensionalidad de los datos mientras se conserva la mayor cantidad posible de la variabilidad original.\n",
    "\n",
    "### ¿Para que se usa PCA?\n",
    "\n",
    "PCA se utiliza para:\n",
    "- Reducción de dimensionalidad\n",
    "- Visualización de datos\n",
    "- Eliminación de ruido\n",
    "- Mejora del rendimiento de los modelos de aprendizaje automático\n",
    "- Identificación de patrones y relaciones en los datos\n",
    "\n",
    "### ¿Como funciona PCA?\n",
    "1. Estandarización de los datos\n",
    "2. Cálculo de la matriz de covarianza\n",
    "3. Cálculo de los valores y vectores propios\n",
    "4. Selección de componentes principales\n",
    "5. Transformación de los datos\n",
    "\n",
    "### ¿Cuales son las ventajas y desventajas de PCA?\n",
    "- Ventajas:\n",
    "  - Reduce la dimensionalidad de los datos, lo que puede mejorar el rendimiento de los modelos.\n",
    "  - Elimina la multicolinealidad entre variables.\n",
    "  - Facilita la visualización de datos en espacios de menor dimensión.\n",
    "- Desventajas:\n",
    "  - Puede perder información importante si se eliminan demasiadas dimensiones.\n",
    "  - Los componentes principales pueden ser difíciles de interpretar.\n",
    "    - Asume relaciones lineales entre variables.\n",
    "\n",
    "### ¿Cuando usar PCA?\n",
    "- Cuando se tiene un conjunto de datos con muchas variables y se desea reducir la dimensionalidad.\n",
    "- Cuando se quiere eliminar la multicolinealidad entre variables.\n",
    "- Cuando se desea visualizar datos en espacios de menor dimensión.\n",
    "\n",
    "### ¿Cuales son los pasos para aplicar PCA?\n",
    "1. Estandarizar los datos.\n",
    "2. Calcular la matriz de covarianza.\n",
    "3. Calcular los valores y vectores propios.\n",
    "4. Seleccionar los componentes principales.\n",
    "5. Transformar los datos.\n",
    "\n",
    "### ¿Como elegir el numero de componentes principales?\n",
    "- Varianza explicada acumulada: Seleccionar el número de componentes que expliquen un porcentaje deseado de la varianza total (por ejemplo, 95%).\n",
    "- Criterio de Kaiser: Seleccionar componentes con valores propios mayores que 1.\n",
    "- Gráfico de codo (Scree plot): Identificar el punto donde la varianza explicada comienza a disminuir significativamente.\n",
    "\n",
    "### ¿Como evaluar la calidad de PCA?\n",
    "- Varianza explicada: Evaluar cuánto de la variabilidad original de los datos es capturada por los componentes principales seleccionados.\n",
    "- Visualización: Utilizar gráficos de biplot o scatter plots para visualizar la distribución de los datos en el espacio de los componentes principales.\n",
    "- Comparación con modelos originales: Evaluar el rendimiento de modelos de aprendizaje automático utilizando los datos transformados por PCA en comparación con los datos originales.\n",
    "\n",
    "### ¿Cuáles son las limitaciones de PCA?\n",
    "- Linealidad: PCA asume que las relaciones entre variables son lineales, lo que puede no ser adecuado para datos con relaciones no lineales.\n",
    "- Interpretabilidad: Los componentes principales pueden ser difíciles de interpretar, ya que son combinaciones lineales de las variables originales.\n",
    "- Sensibilidad a la escala: PCA es sensible a la escala de las variables, por lo que es importante estandarizar los datos antes de aplicar PCA.\n",
    "- Pérdida de información: Al reducir la dimensionalidad, se puede perder información importante si se eliminan demasiadas dimensiones.\n",
    "\n",
    "### ¿Como se aplica PCA en Python?\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "# Estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)  # Número de componentes principales deseados\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "# Convertir a DataFrame para facilitar el manejo\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])\n",
    "``` \n",
    "\n",
    "### ¿Como interpretar los resultados de PCA?\n",
    "- Varianza explicada: Indica cuánto de la variabilidad original de los datos es capturada por cada componente principal.\n",
    "- Cargas de los componentes: Indican la contribución de cada variable original a cada componente principal.\n",
    "- Gráficos de biplot: Permiten visualizar la relación entre las variables originales y los componentes principales.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
