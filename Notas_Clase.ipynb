{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648124a2",
   "metadata": {},
   "source": [
    "# Data cleaning and Preparation\n",
    "\n",
    "## Handling missing data  / Manejo de los datos faltantes\n",
    "\n",
    "### ¿Que son los datos faltantes?\n",
    "\n",
    "Son valores que no se registraron, no se midieron, se perdieron o no aplican para una observación. Pasa por mil razones: encuestas sin responder, sensores que fallan, errores humanos, sistemas que no preguntan lo que debían.\n",
    "\n",
    "### ¿Como se presentan los datos faltantes?\n",
    "\n",
    "- NULL → el rey del “no hay nada aquí”.\n",
    "\n",
    "- NaN (Not a Number) → típico en Python, R y Excel para datos numéricos.\n",
    "\n",
    "- NA → muy usado en R.\n",
    "\n",
    "### ¿Cuales son los tipos de datos faltantes (Segun el origen o el motivo por el que faltan)?\n",
    "\n",
    "- MCAR (Missing Completely At Random - Faltantes completamente al azar)\n",
    "- MAR (Missing At Random - Faltantes al azar condicionado a otras variables)\n",
    "- MNAR (Missing Not At Random - Faltantes no al azar)\n",
    "\n",
    "### ¿Cuales son las tecnicas para manejar datos faltantes?\n",
    "\n",
    "Eliminación por filas (Listwise deletion)\n",
    "Eliminación por columnas\n",
    "Imputación simple (rápida, pero ingenua)\n",
    "Imputación basada en datos (más inteligente)\n",
    "KNN Imputation\n",
    "Imputación estadística avanzada\n",
    "MICE (Multiple Imputation by Chained Equations)\n",
    "\n",
    "\n",
    "### ¿Que es un algoritmo supervisado y no supervisado?\n",
    "- Algoritmos supervisados: Se entrenan con datos etiquetados, es decir, datos que ya tienen la respuesta correcta asociada. El objetivo es aprender una función que pueda predecir la etiqueta correcta para nuevos datos.\n",
    "Ejemplo: \n",
    "Clasificación y regresión. (Decision Trees, Random Forest, SVM, Redes Neuronales, etc.)\n",
    "Ejemplo de uso: Detección de spam, reconocimiento de imágenes, predicción de precios.\n",
    "\n",
    "- Algoritmos no supervisados: Trabajan con datos no etiquetados. El objetivo es encontrar patrones, estructuras o agrupaciones en los datos sin tener una respuesta correcta predefinida.\n",
    "Ejemplo:\n",
    "Clustering y reducción de dimensionalidad. (K-Means, DBSCAN, PCA, etc.)\n",
    "Ejemplo de uso: Segmentación de clientes, detección de anomalías, análisis exploratorio de datos.\n",
    "\n",
    "\n",
    "# Metodos para determinar el numero optimo de clusters\n",
    "\n",
    "# Metodo del Codo (Elbow Method)\n",
    "\n",
    "### ¿Que es el Metodo del Codo?\n",
    "\n",
    "El Método del Codo es una técnica utilizada para determinar el número óptimo de clusters en un conjunto de datos al aplicar algoritmos de clustering como K-Means. Consiste en calcular la inercia (suma de las distancias cuadradas entre los puntos y el centroide del cluster) para diferentes valores de K y graficar estos valores. El punto donde la disminución de la inercia comienza a ser menos pronunciada (formando un \"codo\" en la gráfica) se considera el número óptimo de clusters.\n",
    "\n",
    "### ¿Como aplicar el Metodo del Codo en Python?\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "# Definir el rango de K\n",
    "k_values = range(1, 11)\n",
    "inertia = []\n",
    "# Calcular la inercia para cada K\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "# Graficar el Metodo del Codo\n",
    "plt.plot(k_values, inertia, marker='o')\n",
    "plt.xlabel('Número de clusters (K)')\n",
    "plt.ylabel('Inercia')\n",
    "plt.title('Método del Codo')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### ¿Para que se usa el Metodo del Codo?\n",
    "El Método del Codo se utiliza para:\n",
    "- Determinar el número óptimo de clusters en algoritmos de clustering.\n",
    "- Evaluar la calidad del clustering.\n",
    "- Ayudar en la toma de decisiones sobre la segmentación de datos.\n",
    "\n",
    "\n",
    "### ¿Cuales son las limitaciones del Metodo del Codo?\n",
    "- Subjetividad: La identificación del \"codo\" puede ser subjetiva y variar entre observadores.\n",
    "- No siempre claro: En algunos casos, el gráfico puede no mostrar un codo claro.\n",
    "- Sensible a la escala: La inercia puede verse afectada por la escala de los datos, por lo que es importante estandarizar los datos antes de aplicar el método.\n",
    "\n",
    "### ¿Cuando usar el Metodo del Codo?\n",
    "- Cuando se desea determinar el número óptimo de clusters en un conjunto de datos.\n",
    "- Cuando se utiliza algoritmos de clustering como K-Means.\n",
    "- Cuando se busca evaluar la calidad del clustering.\n",
    "\n",
    "### ¿Como interpretar el grafico del Metodo del Codo?\n",
    "- Identificar el punto donde la disminución de la inercia comienza a ser menos pronunciada.\n",
    "- Este punto indica el número óptimo de clusters.\n",
    "- Si no hay un codo claro, puede ser necesario utilizar otros métodos para determinar el número óptimo de clusters.\n",
    "- Elige el número de clusters justo antes de que, al agregar uno más, la mejora en la inercia (WCSS) deje de ser grande y pase a ser pequeña y parecida en los siguientes valores (Menor a 10%).\n",
    "- Calcula la tasa de cambio porcentual entre cada par de puntos consecutivos en la gráfica de inercia y busca el punto donde esta tasa disminuye significativamente. Ejemplo: Si la tasa de cambio entre K=2 y K=3 es del 30%, pero entre K=3 y K=4 es del 5%, entonces K=3 podría ser una buena elección. (La idea es que sea mayor al 10%).\n",
    "\n",
    "\n",
    "# Coeficiente de Silueta (Silhouette Score)\n",
    "\n",
    "### ¿Que es el Coeficiente de Silueta?\n",
    "\n",
    "El Coeficiente de Silueta es una métrica utilizada para evaluar la calidad de un clustering. Mide qué tan similar es un punto a su propio cluster en comparación con otros clusters. El valor del coeficiente varía entre -1 y 1, donde un valor cercano a 1 indica que el punto está bien agrupado, un valor cercano a 0 indica que el punto está en el límite entre dos clusters, y un valor negativo indica que el punto podría estar mal asignado a su cluster.\n",
    "\n",
    "### ¿Como aplicar el Coeficiente de Silueta en Python?\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "# Definir el número de clusters\n",
    "k = 3\n",
    "# Crear el modelo K-Means\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "# Ajustar el modelo a los datos\n",
    "kmeans.fit(data)\n",
    "# Obtener las etiquetas de los clusters\n",
    "labels = kmeans.labels_\n",
    "# Calcular el Coeficiente de Silueta\n",
    "silhouette_avg = silhouette_score(data, labels)\n",
    "print(f'Coeficiente de Silueta para K={k}: {silhouette_avg}')\n",
    "```\n",
    "\n",
    "### ¿Para que se usa el Coeficiente de Silueta?\n",
    "El Coeficiente de Silueta se utiliza para:\n",
    "- Evaluar la calidad del clustering.\n",
    "- Determinar el número óptimo de clusters.\n",
    "- Comparar diferentes algoritmos de clustering.\n",
    "\n",
    "### ¿Cuales son las limitaciones del Coeficiente de Silueta?\n",
    "- Sensible a la forma de los clusters: Puede no ser adecuado para clusters con formas complejas.\n",
    "- No siempre claro: En algunos casos, el valor del coeficiente puede no proporcionar una indicación clara del número óptimo de clusters.\n",
    "- Requiere etiquetas de clusters: Necesita las etiquetas de los clusters para calcular el coeficiente.\n",
    "\n",
    "### ¿Cuando usar el Coeficiente de Silueta?\n",
    "- Cuando se desea evaluar la calidad del clustering.\n",
    "- Cuando se utiliza algoritmos de clustering como K-Means.\n",
    "- Cuando se busca determinar el número óptimo de clusters.\n",
    "\n",
    "### ¿Como interpretar el valor del Coeficiente de Silueta?\n",
    "- Valores cercanos a 1: Indican que los puntos están bien agrupados dentro de su propio cluster.\n",
    "- Valores cercanos a 0: Indican que los puntos están en el límite entre dos clusters.\n",
    "- Valores negativos: Indican que los puntos podrían estar mal asignados a su cluster.\n",
    "- Comparar los valores del coeficiente para diferentes números de clusters y elegir el número que maximice el valor promedio del coeficiente de silueta.\n",
    "- Un valor promedio del coeficiente de silueta mayor o igual a 0.5 generalmente indica una buena estructura de clustering, mientras que valores por debajo de 0.2 sugieren que los clusters pueden no estar bien definidos.\n",
    "\n",
    "\n",
    "# Índice de Davies-Bouldin\n",
    "\n",
    "### ¿Que es el Índice de Davies-Bouldin?\n",
    "\n",
    "El Índice de Davies-Bouldin es una métrica utilizada para evaluar la calidad de un clustering. Mide la relación entre la dispersión dentro de los clusters y la separación entre los clusters. Un valor más bajo del índice indica una mejor calidad del clustering, ya que sugiere que los clusters están bien separados y tienen baja dispersión interna.\n",
    "\n",
    "### ¿Como aplicar el Índice de Davies-Bouldin en Python?\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.cluster import KMeans\n",
    "# Definir el número de clusters\n",
    "k = 3\n",
    "# Crear el modelo K-Means\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "# Ajustar el modelo a los datos\n",
    "kmeans.fit(data)\n",
    "# Obtener las etiquetas de los clusters\n",
    "labels = kmeans.labels_\n",
    "# Calcular el Índice de Davies-Bouldin\n",
    "db_index = davies_bouldin_score(data, labels)\n",
    "print(f'Índice de Davies-Bouldin para K={k}: {db_index}')\n",
    "```\n",
    "\n",
    "### ¿Para que se usa el Índice de Davies-Bouldin?\n",
    "El Índice de Davies-Bouldin se utiliza para:\n",
    "- Evaluar la calidad del clustering.\n",
    "- Determinar el número óptimo de clusters.\n",
    "- Comparar diferentes algoritmos de clustering.\n",
    "\n",
    "### ¿Cuales son las limitaciones del Índice de Davies-Bouldin?\n",
    "- Sensible a la forma de los clusters: Puede no ser adecuado para clusters con formas complejas.\n",
    "- No siempre claro: En algunos casos, el valor del índice puede no proporcionar una indicación clara del número óptimo de clusters.\n",
    "- Requiere etiquetas de clusters: Necesita las etiquetas de los clusters para calcular el índice.\n",
    "\n",
    "### ¿Cuando usar el Índice de Davies-Bouldin?\n",
    "- Cuando se desea evaluar la calidad del clustering.\n",
    "- Cuando se utiliza algoritmos de clustering como K-Means.\n",
    "- Cuando se busca determinar el número óptimo de clusters.  \n",
    "\n",
    "### ¿Como interpretar el valor del Índice de Davies-Bouldin?\n",
    "- Valores más bajos: Indican una mejor calidad del clustering, con clusters bien separados y baja dispersión interna.\n",
    "- Comparar los valores del índice para diferentes números de clusters y elegir el número que minimice el valor del índice.\n",
    "- Un valor del índice cercano a 0 indica una excelente calidad del clustering, mientras que valores más altos sugieren que los clusters pueden estar mal definidos o solapados.\n",
    "- Elige el número de clusters donde el índice de Davies–Bouldin alcanza su valor más bajo por primera vez y deja de disminuir de forma significativa al aumentar los clusters (Una diferencia de al menos 0.05 entre valores consecutivos).\n",
    "\n",
    "# Indice de Dunn\n",
    "\n",
    "### ¿Que es el Índice de Dunn?\n",
    "\n",
    "El Índice de Dunn es una métrica utilizada para evaluar la calidad de un clustering. Mide la relación entre la distancia mínima entre clusters y el diámetro máximo dentro de los clusters. Un valor más alto del índice indica una mejor calidad del clustering, ya que sugiere que los clusters están bien separados y tienen baja dispersión interna.\n",
    "\n",
    "### ¿Como aplicar el Índice de Dunn en Python?\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import KMeans\n",
    "def dunn_index(data, labels):\n",
    "    unique_clusters = np.unique(labels)\n",
    "    num_clusters = len(unique_clusters)\n",
    "    \n",
    "    # Calcular la distancia mínima entre clusters\n",
    "    min_intercluster_distance = np.inf\n",
    "    for i in range(num_clusters):\n",
    "        for j in range(i + 1, num_clusters):\n",
    "            cluster_i = data[labels == unique_clusters[i]]\n",
    "            cluster_j = data[labels == unique_clusters[j]]\n",
    "            distance = np.min(pairwise_distances(cluster_i, cluster_j))\n",
    "            if distance < min_intercluster_distance:\n",
    "                min_intercluster_distance = distance\n",
    "    \n",
    "    # Calcular el diámetro máximo dentro de los clusters\n",
    "    max_intracluster_diameter = 0\n",
    "    for i in range(num_clusters):\n",
    "        cluster_i = data[labels == unique_clusters[i]]\n",
    "        diameter = np.max(pairwise_distances(cluster_i))\n",
    "        if diameter > max_intracluster_diameter:\n",
    "            max_intracluster_diameter = diameter\n",
    "    \n",
    "    # Calcular el Índice de Dunn\n",
    "    dunn_index_value = min_intercluster_distance / max_intracluster_diameter\n",
    "    return dunn_index_value\n",
    "# Definir el número de clusters\n",
    "k = 3\n",
    "# Crear el modelo K-Means\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "# Ajustar el modelo a los datos\n",
    "kmeans.fit(data)\n",
    "# Obtener las etiquetas de los clusters\n",
    "labels = kmeans.labels_\n",
    "# Calcular el Índice de Dunn\n",
    "dunn_index_value = dunn_index(data, labels)\n",
    "print(f'Índice de Dunn para K={k}: {dunn_index_value}')\n",
    "```\n",
    "\n",
    "### ¿Para que se usa el Índice de Dunn?\n",
    "El Índice de Dunn se utiliza para:\n",
    "- Evaluar la calidad del clustering.\n",
    "- Determinar el número óptimo de clusters.\n",
    "- Comparar diferentes algoritmos de clustering.\n",
    "\n",
    "### ¿Cuales son las limitaciones del Índice de Dunn?\n",
    "- Sensible a la forma de los clusters: Puede no ser adecuado para clusters con formas complejas.\n",
    "- No siempre claro: En algunos casos, el valor del índice puede no proporcionar una indicación clara del número óptimo de clusters.\n",
    "- Requiere etiquetas de clusters: Necesita las etiquetas de los clusters para calcular el índice.\n",
    "\n",
    "### ¿Cuando usar el Índice de Dunn?\n",
    "- Cuando se desea evaluar la calidad del clustering.\n",
    "- Cuando se utiliza algoritmos de clustering como K-Means.\n",
    "- Cuando se busca determinar el número óptimo de clusters.\n",
    "\n",
    "### ¿Como interpretar el valor del Índice de Dunn?\n",
    "- Valores más altos: Indican una mejor calidad del clustering, con clusters bien separados y baja dispersión interna.\n",
    "- Comparar los valores del índice para diferentes números de clusters y elegir el número que maximice el valor del índice.\n",
    "- Un valor del índice mayor a 1 generalmente indica una buena calidad del clustering, mientras que valores menores a 0.5 sugieren que los clusters pueden no estar bien definidos o solapados.\n",
    "- Elige el número de clusters donde el índice de Dunn alcanza su valor más alto por primera vez y se mantiene estable al aumentar los clusters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# K-Means Clustering\n",
    "\n",
    "### ¿Que es K-Means?\n",
    "\n",
    "K-Means es un algoritmo de aprendizaje no supervisado que agrupa datos en K clusters basándose en la similitud de sus características. El objetivo es minimizar la variabilidad dentro de cada cluster y maximizar la variabilidad entre clusters.\n",
    "\n",
    "### ¿Para que se usa K-Means?\n",
    "K-Means se utiliza para:\n",
    "- Segmentación de clientes\n",
    "- Agrupación de documentos\n",
    "- Detección de anomalías\n",
    "- Compresión de imágenes\n",
    "- Análisis exploratorio de datos\n",
    "\n",
    "### ¿Como aplicar K-Means en Python?\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "# Definir el número de clusters\n",
    "k = 3\n",
    "# Crear el modelo K-Means\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "# Ajustar el modelo a los datos\n",
    "kmeans.fit(data)\n",
    "# Obtener las etiquetas de los clusters\n",
    "labels = kmeans.labels_\n",
    "# Obtener los centroides de los clusters\n",
    "centroids = kmeans.cluster_centers_\n",
    "``` \n",
    "\n",
    "# DBSCAN Clustering\n",
    "\n",
    "### ¿Que es DBSCAN?\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de clustering basado en densidad que agrupa puntos que están densamente conectados y marca como ruido los puntos que están en regiones de baja densidad.\n",
    "\n",
    "### ¿Como aplicar DBSCAN en Python?\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "# Definir los parámetros de DBSCAN\n",
    "epsilon = 0.5  # Distancia máxima entre dos puntos para que se consideren vecinos\n",
    "min_samples = 5  # Número mínimo de puntos necesarios para formar un cluster\n",
    "# Crear el modelo DBSCAN\n",
    "dbscan = DBSCAN(eps=epsilon, min_samples=min_samples)\n",
    "# Ajustar el modelo a los datos\n",
    "dbscan.fit(data)\n",
    "# Obtener las etiquetas de los clusters\n",
    "labels = dbscan.labels_\n",
    "```\n",
    "\n",
    "### ¿Para que se usa DBSCAN?\n",
    "DBSCAN se utiliza para:\n",
    "- Detección de anomalías\n",
    "- Agrupación de datos espaciales\n",
    "- Análisis de redes sociales\n",
    "- Segmentación de clientes\n",
    "- Análisis exploratorio de datos\n",
    "\n",
    "### ¿Cual es la diferencia entre K-Means y DBSCAN?\n",
    "- K-Means requiere que el número de clusters (K) sea especificado de antemano, mientras que DBSCAN determina automáticamente el número de clusters basándose en la densidad de los datos.\n",
    "- K-Means tiende a formar clusters de forma esférica y puede ser sensible a outliers, mientras que DBSCAN puede formar clusters de forma arbitraria y es más robusto frente a outliers.\n",
    "- K-Means utiliza la media de los puntos para definir el centro del cluster, mientras que DBSCAN utiliza la densidad de puntos para definir los clusters.\n",
    "\n",
    "### ¿Como idenfico cual usar?\n",
    "- Si se conoce el número de clusters y los datos son relativamente limpios, K-Means puede ser una buena opción.\n",
    "- Si los datos contienen ruido o outliers, o si los clusters tienen formas arbitrarias, DBSCAN puede ser más adecuado.\n",
    "\n",
    "### ¿Como se determina el numero de clusters en K-Means?\n",
    "- Método del codo (Elbow Method)\n",
    "- Silhouette Score\n",
    "- Gap Statistic\n",
    "\n",
    "### ¿Como se determina el numero de clusters en DBSCAN?\n",
    "- Parámetro epsilon (ε): Distancia máxima entre dos puntos para que se consideren vecinos\n",
    "- Parámetro minPts: Número mínimo de puntos necesarios para formar un cluster denso\n",
    "\n",
    "### ¿Como se determina epsilon y minPts en DBSCAN?\n",
    "- Epsilon (ε): Se puede determinar utilizando un gráfico de distancia k-vecino (k-distance graph) para identificar un punto de inflexión.\n",
    "- minPts: Una regla común es establecer minPts al menos igual a la dimensionalidad de los datos más uno (minPts ≥ D + 1).\n",
    "\n",
    "\n",
    "# ANN\n",
    "\n",
    "### ¿Que es ANN?\n",
    "\n",
    "ANN (Artificial Neural Network - Red Neuronal Artificial) es un modelo de aprendizaje automático inspirado en la estructura y funcionamiento del cerebro humano. Está compuesto por capas de nodos (neuronas) que procesan y transmiten información.\n",
    "\n",
    "### ¿Para que se usa ANN?\n",
    "\n",
    "ANN se utiliza para:\n",
    "- Clasificación\n",
    "- Regresión\n",
    "- Reconocimiento de patrones\n",
    "- Procesamiento de imágenes\n",
    "- Procesamiento de lenguaje natural\n",
    "- Juegos y simulaciones\n",
    "\n",
    "### ¿Como funciona ANN?\n",
    "1. Entrada de datos: Los datos se introducen en la capa de entrada.\n",
    "2. Propagación hacia adelante: Los datos se procesan a través de las capas ocultas utilizando funciones de activación.\n",
    "3. Cálculo del error: Se calcula la diferencia entre la salida predicha y la salida real.\n",
    "4. Retropropagación: El error se propaga hacia atrás para ajustar los pesos de las conexiones.\n",
    "5. Repetición: El proceso se repite hasta que el modelo converge.\n",
    "\n",
    "### ¿Cuales son las ventajas y desventajas de ANN?\n",
    "- Ventajas:\n",
    "  - Capacidad para modelar relaciones complejas y no lineales.\n",
    "  - Adaptabilidad a diferentes tipos de datos.\n",
    "  - Capacidad para aprender características automáticamente.\n",
    "- Desventajas:\n",
    "  - Requiere grandes cantidades de datos para entrenar eficazmente.\n",
    "  - Puede ser propenso al sobreajuste.\n",
    "  - Interpretabilidad limitada.\n",
    "\n",
    "### ¿Cuando usar ANN?\n",
    "- Cuando se tienen grandes cantidades de datos y se desea modelar relaciones complejas.\n",
    "- Cuando se trabaja con datos no estructurados como imágenes, texto o audio.\n",
    "- Cuando se busca automatizar la extracción de características.\n",
    "\n",
    "### ¿Cuales son los pasos para aplicar ANN?\n",
    "1. Preparar los datos.\n",
    "2. Definir la arquitectura de la red.\n",
    "3. Compilar el modelo.\n",
    "4. Entrenar el modelo.\n",
    "5. Evaluar el modelo.\n",
    "6. Hacer predicciones.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PCA (Principal Component Analysis - Análisis de Componentes Principales)\n",
    "\n",
    "### ¿Que es PCA?\n",
    "\n",
    "PCA es una técnica estadística que transforma un conjunto de variables posiblemente correlacionadas en un conjunto de variables linealmente no correlacionadas llamadas componentes principales. El objetivo principal de PCA es reducir la dimensionalidad de los datos mientras se conserva la mayor cantidad posible de la variabilidad original.\n",
    "\n",
    "### ¿Para que se usa PCA?\n",
    "\n",
    "PCA se utiliza para:\n",
    "- Reducción de dimensionalidad\n",
    "- Visualización de datos\n",
    "- Eliminación de ruido\n",
    "- Mejora del rendimiento de los modelos de aprendizaje automático\n",
    "- Identificación de patrones y relaciones en los datos\n",
    "\n",
    "### ¿Como funciona PCA?\n",
    "1. Estandarización de los datos\n",
    "2. Cálculo de la matriz de covarianza\n",
    "3. Cálculo de los valores y vectores propios\n",
    "4. Selección de componentes principales\n",
    "5. Transformación de los datos\n",
    "\n",
    "### ¿Cuales son las ventajas y desventajas de PCA?\n",
    "- Ventajas:\n",
    "  - Reduce la dimensionalidad de los datos, lo que puede mejorar el rendimiento de los modelos.\n",
    "  - Elimina la multicolinealidad entre variables.\n",
    "  - Facilita la visualización de datos en espacios de menor dimensión.\n",
    "- Desventajas:\n",
    "  - Puede perder información importante si se eliminan demasiadas dimensiones.\n",
    "  - Los componentes principales pueden ser difíciles de interpretar.\n",
    "    - Asume relaciones lineales entre variables.\n",
    "\n",
    "### ¿Cuando usar PCA?\n",
    "- Cuando se tiene un conjunto de datos con muchas variables y se desea reducir la dimensionalidad.\n",
    "- Cuando se quiere eliminar la multicolinealidad entre variables.\n",
    "- Cuando se desea visualizar datos en espacios de menor dimensión.\n",
    "\n",
    "### ¿Cuales son los pasos para aplicar PCA?\n",
    "1. Estandarizar los datos.\n",
    "2. Calcular la matriz de covarianza.\n",
    "3. Calcular los valores y vectores propios.\n",
    "4. Seleccionar los componentes principales.\n",
    "5. Transformar los datos.\n",
    "\n",
    "### ¿Como elegir el numero de componentes principales?\n",
    "- Varianza explicada acumulada: Seleccionar el número de componentes que expliquen un porcentaje deseado de la varianza total (por ejemplo, 95%).\n",
    "- Criterio de Kaiser: Seleccionar componentes con valores propios mayores que 1.\n",
    "- Gráfico de codo (Scree plot): Identificar el punto donde la varianza explicada comienza a disminuir significativamente.\n",
    "\n",
    "### ¿Como evaluar la calidad de PCA?\n",
    "- Varianza explicada: Evaluar cuánto de la variabilidad original de los datos es capturada por los componentes principales seleccionados.\n",
    "- Visualización: Utilizar gráficos de biplot o scatter plots para visualizar la distribución de los datos en el espacio de los componentes principales.\n",
    "- Comparación con modelos originales: Evaluar el rendimiento de modelos de aprendizaje automático utilizando los datos transformados por PCA en comparación con los datos originales.\n",
    "\n",
    "### ¿Cuáles son las limitaciones de PCA?\n",
    "- Linealidad: PCA asume que las relaciones entre variables son lineales, lo que puede no ser adecuado para datos con relaciones no lineales.\n",
    "- Interpretabilidad: Los componentes principales pueden ser difíciles de interpretar, ya que son combinaciones lineales de las variables originales.\n",
    "- Sensibilidad a la escala: PCA es sensible a la escala de las variables, por lo que es importante estandarizar los datos antes de aplicar PCA.\n",
    "- Pérdida de información: Al reducir la dimensionalidad, se puede perder información importante si se eliminan demasiadas dimensiones.\n",
    "\n",
    "### ¿Como se aplica PCA en Python?\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "# Estandarizar los datos\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "# Aplicar PCA\n",
    "pca = PCA(n_components=2)  # Número de componentes principales deseados\n",
    "data_pca = pca.fit_transform(data_scaled)\n",
    "# Convertir a DataFrame para facilitar el manejo\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])\n",
    "``` \n",
    "\n",
    "### ¿Como interpretar los resultados de PCA?\n",
    "- Varianza explicada: Indica cuánto de la variabilidad original de los datos es capturada por cada componente principal.\n",
    "- Cargas de los componentes: Indican la contribución de cada variable original a cada componente principal.\n",
    "- Gráficos de biplot: Permiten visualizar la relación entre las variables originales y los componentes principales.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etarea6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
